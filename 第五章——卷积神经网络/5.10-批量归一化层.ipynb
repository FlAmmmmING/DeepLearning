{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 从零开始实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3865b1b15250ff55"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-21T13:30:36.136315900Z",
     "start_time": "2024-11-21T13:30:30.940129500Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import library.d2lzh_pytorch as d2l\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # 判断当前模式是训练模式还是预测模式\n",
    "    if not is_training:\n",
    "        # 如果是在预测模式下,直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 使用全连接层，计算特征维上的均值和方差\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积层的情况,计算通道维上(axis=1)的均值和方差。这我们需要保持\n",
    "            # X的形状以便后面可以做广播运算\n",
    "            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n",
    "            # 训练模式下用当前的均值和方差标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1 - momentum) * var\n",
    "    Y = gamma * X_hat + beta # 拉伸和偏移\n",
    "    return Y, moving_mean, moving_var"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T13:59:33.024161Z",
     "start_time": "2024-11-21T13:59:33.007646900Z"
    }
   },
   "id": "eac2156b4fe7678f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数,分别初始化成0和1\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量,全在内存上初始化成0\n",
    "        self.moving_mean = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_var = nn.Parameter(torch.zeros(shape))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上,将moving_mean和moving_var复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用用.eval()后设成false\n",
    "        Y, self.moving_mean, self.moving_var =  batch_norm(self.training,  X, self.gamma, self.beta, self.moving_mean,  self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T13:59:33.491584200Z",
     "start_time": "2024-11-21T13:59:33.481066200Z"
    }
   },
   "id": "caa4705d0e7d7d6b",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LeNet 添加BatchNord"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d367268d1cde867"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, 5),\n",
    "    BatchNorm(6, 4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(6, 16, 5),\n",
    "    BatchNorm(16, num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(16*4*4, 120),\n",
    "    BatchNorm(120, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    BatchNorm(84, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T13:59:34.176530Z",
     "start_time": "2024-11-21T13:59:34.153444300Z"
    }
   },
   "id": "ca297f081543a53",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 256  \n",
    "train_iter, test_iter =  d2l.load_data_fashion_mnist(batch_size=batch_size)  \n",
    "lr, num_epochs = 0.001, 5  \n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)  \n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "398e686c03248419",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 快速实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "104f3198f5a0ac0f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "net = nn.Sequential(  \n",
    "    nn.Conv2d(1, 6, 5), # in_channels, out_channels,  kernel_size  \n",
    "    nn.BatchNorm2d(6),  \n",
    "    nn.Sigmoid(),  \n",
    "    nn.MaxPool2d(2, 2), # kernel_size, stride  \n",
    "    nn.Conv2d(6, 16, 5),  \n",
    "    nn.BatchNorm2d(16),  \n",
    "    nn.Sigmoid(),  \n",
    "    nn.MaxPool2d(2, 2),  \n",
    "    d2l.FlattenLayer(),  \n",
    "    nn.Linear(16*4*4, 120),  \n",
    "    nn.BatchNorm1d(120),  \n",
    "    nn.Sigmoid(),  \n",
    "    nn.Linear(120, 84),  \n",
    "    nn.BatchNorm1d(84),  \n",
    "    nn.Sigmoid(),  \n",
    "    nn.Linear(84, 10)  \n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T14:01:16.561787300Z",
     "start_time": "2024-11-21T14:01:16.541757600Z"
    }
   },
   "id": "ecca41f0eac5c8ed",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 1.0016, train acc 0.779, test acc 0.811,  time 7.0 sec\n",
      "epoch 2, loss 0.2300, train acc 0.862, test acc 0.800,  time 7.7 sec\n",
      "epoch 3, loss 0.1230, train acc 0.878, test acc 0.866,  time 7.9 sec\n",
      "epoch 4, loss 0.0826, train acc 0.886, test acc 0.852,  time 7.0 sec\n",
      "epoch 5, loss 0.0617, train acc 0.893, test acc 0.868,  time 7.1 sec\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256  \n",
    "train_iter, test_iter =  d2l.load_data_fashion_mnist(batch_size=batch_size)  \n",
    "lr, num_epochs = 0.001, 5  \n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)  \n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-21T14:01:59.614055900Z",
     "start_time": "2024-11-21T14:01:23.006714400Z"
    }
   },
   "id": "735d2d64c996af33",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4a85f246bde06677"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
